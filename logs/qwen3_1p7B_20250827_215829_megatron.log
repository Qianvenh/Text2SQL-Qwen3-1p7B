W0827 21:58:30.269000 3744497 site-packages/torch/distributed/run.py:793] 
W0827 21:58:30.269000 3744497 site-packages/torch/distributed/run.py:793] *****************************************
W0827 21:58:30.269000 3744497 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0827 21:58:30.269000 3744497 site-packages/torch/distributed/run.py:793] *****************************************
[WARNING|2025-08-27 21:58:34] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-08-27 21:58:34,836] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 21:58:34,840] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 21:58:35,133] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 21:58:35,773] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
[2025-08-27 21:58:42,113] [INFO] [mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:42,117] [INFO] [mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:42,118] [INFO] [mcore_adapter.initialize]: Initializing mpu on device cuda:2
[INFO     | mcore_adapter.initialize]: Initializing mpu on device cuda:2
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
[2025-08-27 21:58:43,104] [INFO] [mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:43,108] [INFO] [mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:43,110] [INFO] [mcore_adapter.initialize]: Initializing mpu on device cuda:1
[INFO     | mcore_adapter.initialize]: Initializing mpu on device cuda:1
[2025-08-27 21:58:43,261] [INFO] [mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:43,267] [INFO] [mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:43,268] [INFO] [mcore_adapter.initialize]: Initializing mpu on device cuda:0
[INFO     | mcore_adapter.initialize]: Initializing mpu on device cuda:0
[2025-08-27 21:58:43,312] [INFO] [mcore_adapter.initialize]: initialized tensor model parallel with size 2
[INFO     | mcore_adapter.initialize]: initialized tensor model parallel with size 2
[2025-08-27 21:58:43,312] [INFO] [mcore_adapter.initialize]: initialized tensor model parallel with size 2
[2025-08-27 21:58:43,312] [INFO] [mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[INFO     | mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[INFO     | mcore_adapter.initialize]: initialized tensor model parallel with size 2
[2025-08-27 21:58:43,313] [INFO] [mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[INFO     | mcore_adapter.initialize]: initialized pipeline model parallel with size 1
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/transformer_engine/pytorch/module/base.py:850: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/transformer_engine/pytorch/module/base.py:850: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
[2025-08-27 21:58:43,528] [INFO] [mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (0, 0, 0): 860349440
[INFO     | mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (0, 0, 0): 860349440
[2025-08-27 21:58:43,528] [INFO] [mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[INFO     | mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[2025-08-27 21:58:43,614] [INFO] [mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (0, 0, 0): 860349440
[INFO     | mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (0, 0, 0): 860349440
[2025-08-27 21:58:43,614] [INFO] [mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[INFO     | mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[2025-08-27 21:58:44,134] [INFO] [mcore_adapter.models.converter.model_converter]: End converting, cost: 0.605s
[INFO     | mcore_adapter.models.converter.model_converter]: End converting, cost: 0.605s
[2025-08-27 21:58:44,290] [INFO] [mcore_adapter.models.converter.model_converter]: End converting, cost: 0.676s
[INFO     | mcore_adapter.models.converter.model_converter]: End converting, cost: 0.676s
[2025-08-27 21:58:44,457] [INFO] [mcore_adapter.models.model_factory]: End loading, cost: 2.340s
[INFO     | mcore_adapter.models.model_factory]: End loading, cost: 2.340s
[rank2]:[W827 21:58:44.197939524 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[2025-08-27 21:58:44,522] [INFO] [mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.auto.modeling_auto]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:44,527] [INFO] [mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[INFO     | mcore_adapter.models.model_config]: Did not find /home/qianwenhao/LLM/Qwen3-1.7B/mca_config.json, loading HuggingFace config from /home/qianwenhao/LLM/Qwen3-1.7B
[2025-08-27 21:58:44,528] [INFO] [mcore_adapter.initialize]: Initializing mpu on device cuda:3
[INFO     | mcore_adapter.initialize]: Initializing mpu on device cuda:3
[2025-08-27 21:58:44,579] [INFO] [mcore_adapter.initialize]: initialized tensor model parallel with size 2
[INFO     | mcore_adapter.initialize]: initialized tensor model parallel with size 2
[2025-08-27 21:58:44,579] [INFO] [mcore_adapter.initialize]: initialized tensor model parallel with size 2
[2025-08-27 21:58:44,579] [INFO] [mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[INFO     | mcore_adapter.initialize]: initialized tensor model parallel with size 2
[INFO     | mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[2025-08-27 21:58:44,580] [INFO] [mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[INFO     | mcore_adapter.initialize]: initialized pipeline model parallel with size 1
[2025-08-27 21:58:44,621] [INFO] [mcore_adapter.models.model_factory]: End loading, cost: 1.354s
[INFO     | mcore_adapter.models.model_factory]: End loading, cost: 1.354s
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/transformer_engine/pytorch/module/base.py:850: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/transformer_engine/pytorch/module/base.py:850: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
[INFO|2025-08-27 21:58:44] llamafactory.data.loader:143 >> Loading dataset text2sql.json...
[2025-08-27 21:58:44,727] [INFO] [mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (1, 0, 0): 860349440
[INFO     | mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (1, 0, 0): 860349440
[2025-08-27 21:58:44,727] [INFO] [mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[INFO     | mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[2025-08-27 21:58:44,756] [INFO] [mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (1, 0, 0): 860349440
[INFO     | mcore_adapter.models.model_factory]: number of parameters on (tensor, pipeline, expert) model parallel rank (1, 0, 0): 860349440
[2025-08-27 21:58:44,757] [INFO] [mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[INFO     | mcore_adapter.models.converter.model_converter]: Begin converting mca state dict from hf ckpt...
[2025-08-27 21:58:45,371] [INFO] [mcore_adapter.models.converter.model_converter]: End converting, cost: 0.645s
[INFO     | mcore_adapter.models.converter.model_converter]: End converting, cost: 0.645s
[2025-08-27 21:58:45,552] [INFO] [mcore_adapter.models.converter.model_converter]: End converting, cost: 0.795s
[INFO     | mcore_adapter.models.converter.model_converter]: End converting, cost: 0.795s
[2025-08-27 21:58:45,684] [INFO] [mcore_adapter.models.model_factory]: End loading, cost: 1.157s
[INFO     | mcore_adapter.models.model_factory]: End loading, cost: 1.157s
[rank3]:[W827 21:58:45.409662367 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[2025-08-27 21:58:45,944] [INFO] [mcore_adapter.models.model_factory]: End loading, cost: 2.836s
[INFO     | mcore_adapter.models.model_factory]: End loading, cost: 2.836s
[rank1]:[W827 21:58:46.697364847 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 0/126285 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 583/126285 [00:00<00:30, 4160.89 examples/s]Converting format of dataset (num_proc=16):  13%|█▎        | 16637/126285 [00:00<00:01, 83027.13 examples/s]Converting format of dataset (num_proc=16):  30%|██▉       | 37373/126285 [00:00<00:00, 134368.50 examples/s]Converting format of dataset (num_proc=16):  46%|████▌     | 58101/126285 [00:00<00:00, 161323.15 examples/s]Converting format of dataset (num_proc=16):  63%|██████▎   | 79425/126285 [00:00<00:00, 179270.83 examples/s]Converting format of dataset (num_proc=16):  79%|███████▉  | 100217/126285 [00:00<00:00, 188705.99 examples/s]Converting format of dataset (num_proc=16):  95%|█████████▌| 119987/126285 [00:00<00:00, 181765.43 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 126285/126285 [00:01<00:00, 107161.40 examples/s]
[rank0]:[W827 21:58:47.293151513 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/126285 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 1000/126285 [00:01<03:20, 623.31 examples/s]Running tokenizer on dataset (num_proc=16):   2%|▏         | 2000/126285 [00:01<01:31, 1365.58 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 4000/126285 [00:01<00:39, 3111.94 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 7000/126285 [00:02<00:19, 6029.89 examples/s]Running tokenizer on dataset (num_proc=16):   7%|▋         | 9000/126285 [00:02<00:15, 7693.00 examples/s]Running tokenizer on dataset (num_proc=16):  10%|▉         | 12000/126285 [00:02<00:10, 10959.14 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 15000/126285 [00:02<00:08, 13354.47 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▍        | 18000/126285 [00:02<00:06, 16037.70 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 21000/126285 [00:02<00:05, 18410.16 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 24000/126285 [00:02<00:04, 20546.59 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██▏       | 27000/126285 [00:02<00:04, 20273.71 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 30000/126285 [00:03<00:04, 21279.38 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 35000/126285 [00:03<00:03, 27530.03 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 39000/126285 [00:03<00:03, 27975.50 examples/s]Running tokenizer on dataset (num_proc=16):  34%|███▍      | 43000/126285 [00:03<00:03, 24515.57 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 49000/126285 [00:03<00:02, 30011.58 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 53000/126285 [00:03<00:02, 28232.41 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 56000/126285 [00:03<00:02, 25225.35 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 61000/126285 [00:04<00:02, 30278.81 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████▏    | 65000/126285 [00:04<00:02, 29978.48 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▍    | 69000/126285 [00:04<00:02, 25264.80 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 76000/126285 [00:04<00:01, 33124.74 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 80000/126285 [00:04<00:01, 24319.95 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 86000/126285 [00:04<00:01, 28933.11 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 89893/126285 [00:05<00:01, 30827.95 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 93786/126285 [00:05<00:01, 26972.62 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 97572/126285 [00:05<00:01, 27371.74 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 101465/126285 [00:05<00:00, 29755.99 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▎ | 105465/126285 [00:05<00:00, 28285.79 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 109357/126285 [00:05<00:00, 21863.74 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 112357/126285 [00:06<00:00, 20943.11 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████▏| 115249/126285 [00:06<00:00, 22102.32 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 117928/126285 [00:06<00:00, 20358.71 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 120607/126285 [00:06<00:00, 15725.77 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 123393/126285 [00:09<00:01, 2798.55 examples/s] Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 125393/126285 [00:18<00:01, 804.66 examples/s] Running tokenizer on dataset (num_proc=16): 100%|██████████| 126285/126285 [00:22<00:00, 5576.23 examples/s]
training example:
input_ids:
[151644, 8948, 198, 22043, 279, 4625, 10802, 323, 279, 1196, 3405, 11, 6923, 279, 12159, 7870, 3239, 13, 151645, 198, 151644, 872, 198, 78045, 3540, 35839, 921, 22599, 14363, 6625, 8987, 320, 29041, 8987, 842, 9221, 11, 829, 15762, 11, 5537, 15762, 1215, 39518, 12496, 6625, 8987, 320, 29041, 8987, 842, 11, 829, 11, 5537, 8, 14710, 320, 16, 11, 364, 13079, 49628, 516, 364, 25221, 4567, 320, 17, 11, 364, 62502, 9082, 516, 364, 25018, 4667, 30776, 14363, 44788, 47067, 320, 29041, 842, 9221, 11, 6625, 8987, 842, 9221, 11, 8123, 25272, 11, 6278, 4164, 28543, 1215, 39518, 12496, 44788, 47067, 320, 29041, 842, 11, 6625, 8987, 842, 11, 8123, 11, 6278, 4164, 8, 14710, 320, 16, 11, 220, 16, 11, 220, 16, 17, 15, 11, 364, 17, 15, 17, 16, 12, 15, 16, 12, 15, 16, 4567, 320, 17, 11, 220, 16, 11, 220, 16, 20, 15, 11, 364, 17, 15, 17, 16, 12, 15, 17, 12, 15, 16, 4567, 320, 18, 11, 220, 17, 11, 220, 16, 23, 15, 11, 364, 17, 15, 17, 16, 12, 15, 16, 12, 15, 16, 1157, 78045, 52428, 921, 3838, 374, 279, 2790, 8123, 315, 44788, 6088, 553, 1817, 6625, 8987, 11, 10615, 553, 6625, 8987, 5267, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 4858, 6625, 8987, 842, 11, 829, 11, 30735, 74706, 8, 438, 2790, 26941, 4295, 44788, 47067, 13069, 6625, 8987, 6197, 44788, 47067, 73270, 8987, 842, 284, 6625, 8987, 73270, 8987, 842, 26870, 7710, 6625, 8987, 842, 11, 829, 15520, 7710, 2790, 26941, 16089, 26, 151645, 198]
inputs:
<|im_start|>system
Given the database schema and the user question, generate the corresponding SQL query.<|im_end|>
<|im_start|>user
\[SCHEMA]
CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');
\[QUESTION]
What is the total volume of timber sold by each salesperson, sorted by salesperson?
<|im_end|>
<|im_start|>assistant
<think>

</think>

SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 4858, 6625, 8987, 842, 11, 829, 11, 30735, 74706, 8, 438, 2790, 26941, 4295, 44788, 47067, 13069, 6625, 8987, 6197, 44788, 47067, 73270, 8987, 842, 284, 6625, 8987, 73270, 8987, 842, 26870, 7710, 6625, 8987, 842, 11, 829, 15520, 7710, 2790, 26941, 16089, 26, 151645, 198]
labels:
<think>

</think>

SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;<|im_end|>

/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/mcore_adapter/trainer/trainer.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `McaTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[2025-08-27 21:59:11,674] [WARNING] [mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[WARNING  | mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]: ***** Running training *****
[INFO     | mcore_adapter.trainer.trainer]: ***** Running training *****
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Num examples = 126,285
[INFO     | mcore_adapter.trainer.trainer]:   Num examples = 126,285
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Num Epochs = 3
[INFO     | mcore_adapter.trainer.trainer]:   Num Epochs = 3
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[INFO     | mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO     | mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[INFO     | mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[2025-08-27 21:59:11,704] [INFO] [mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[INFO     | mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[2025-08-27 21:59:11,706] [INFO] [mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[INFO     | mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
  0%|          | 0/7890 [00:00<?, ?it/s]/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/mcore_adapter/trainer/trainer.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `McaTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/mcore_adapter/trainer/trainer.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `McaTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/mcore_adapter/trainer/trainer.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `McaTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[2025-08-27 21:59:11,805] [WARNING] [mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[WARNING  | mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[2025-08-27 21:59:11,807] [WARNING] [mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[2025-08-27 21:59:11,807] [WARNING] [mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[WARNING  | mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[WARNING  | mcore_adapter.trainer.trainer]: Currently, train dataloader drop_last must be set to True!
[2025-08-27 21:59:11,822] [INFO] [mcore_adapter.trainer.trainer]: ***** Running training *****
[INFO     | mcore_adapter.trainer.trainer]: ***** Running training *****
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Num examples = 126,285
[INFO     | mcore_adapter.trainer.trainer]:   Num examples = 126,285
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Num Epochs = 3
[INFO     | mcore_adapter.trainer.trainer]:   Num Epochs = 3
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[INFO     | mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO     | mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[INFO     | mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[2025-08-27 21:59:11,823] [INFO] [mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[INFO     | mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[INFO     | mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]: ***** Running training *****
[INFO     | mcore_adapter.trainer.trainer]: ***** Running training *****
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Num examples = 126,285
[INFO     | mcore_adapter.trainer.trainer]:   Num examples = 126,285
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Num Epochs = 3
[INFO     | mcore_adapter.trainer.trainer]:   Num Epochs = 3
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[INFO     | mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO     | mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[INFO     | mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[INFO     | mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]: ***** Running training *****
[INFO     | mcore_adapter.trainer.trainer]: ***** Running training *****
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Num examples = 126,285
[INFO     | mcore_adapter.trainer.trainer]:   Num examples = 126,285
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Num Epochs = 3
[INFO     | mcore_adapter.trainer.trainer]:   Num Epochs = 3
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[INFO     | mcore_adapter.trainer.trainer]:   Instantaneous batch size per device = 6
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO     | mcore_adapter.trainer.trainer]:   Total train batch size (w. parallel, distributed & accumulation) = 48
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[INFO     | mcore_adapter.trainer.trainer]:   Gradient Accumulation steps = 4
[2025-08-27 21:59:11,824] [INFO] [mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[INFO     | mcore_adapter.trainer.trainer]:   Total optimization steps = 7,890
[2025-08-27 21:59:11,825] [INFO] [mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[INFO     | mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[2025-08-27 21:59:11,825] [INFO] [mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
[INFO     | mcore_adapter.trainer.trainer]:   Number of trainable parameters = 860,349,440
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:653: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup
  warnings.warn(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:653: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup
  warnings.warn(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:653: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup
  warnings.warn(
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:653: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup
  warnings.warn(
[WARNING  | megatron.core.rerun_state_machine]: Implicit initialization of Rerun State Machine!
[WARNING  | megatron.core.rerun_state_machine]: Implicit initialization of Rerun State Machine!
[WARNING  | megatron.core.rerun_state_machine]: RerunStateMachine initialized in mode RerunMode.DISABLED
[WARNING  | megatron.core.rerun_state_machine]: Implicit initialization of Rerun State Machine!
[WARNING  | megatron.core.rerun_state_machine]: Implicit initialization of Rerun State Machine!
  0%|          | 1/7890 [00:08<18:08:57,  8.28s/it]  0%|          | 2/7890 [00:11<11:48:05,  5.39s/it]  0%|          | 3/7890 [00:12<7:31:41,  3.44s/it]   0%|          | 4/7890 [00:16<7:31:20,  3.43s/it]  0%|          | 5/7890 [00:17<5:41:58,  2.60s/it]  0%|          | 6/7890 [00:18<4:36:52,  2.11s/it]  0%|          | 7/7890 [00:19<3:53:53,  1.78s/it]  0%|          | 8/7890 [00:20<3:27:14,  1.58s/it]  0%|          | 9/7890 [00:24<5:02:53,  2.31s/it]  0%|          | 10/7890 [00:27<5:43:12,  2.61s/it]  0%|          | 11/7890 [00:29<4:42:03,  2.15s/it]  0%|          | 12/7890 [00:30<4:25:13,  2.02s/it]  0%|          | 13/7890 [00:34<5:23:50,  2.47s/it]  0%|          | 14/7890 [00:36<5:14:36,  2.40s/it]  0%|          | 15/7890 [00:37<4:34:14,  2.09s/it]  0%|          | 16/7890 [00:38<3:54:14,  1.78s/it]  0%|          | 17/7890 [00:41<4:22:06,  2.00s/it]  0%|          | 18/7890 [00:42<3:46:10,  1.72s/it]  0%|          | 19/7890 [00:44<4:11:34,  1.92s/it]  0%|          | 20/7890 [00:45<3:39:00,  1.67s/it]  0%|          | 21/7890 [00:47<3:20:36,  1.53s/it]  0%|          | 22/7890 [00:48<3:04:42,  1.41s/it]  0%|          | 23/7890 [00:51<4:30:49,  2.07s/it]  0%|          | 24/7890 [00:53<4:05:38,  1.87s/it]  0%|          | 25/7890 [00:54<3:35:21,  1.64s/it]                                                   {'loss': 1.4157, 'grad_norm': 4.160612871067269, 'learning_rate': 6.329113924050633e-06, 'skipped_iter': 0, 'num_zeros_in_grad': 92315.0, 'epoch': 0.01}
  0%|          | 25/7890 [00:54<3:35:21,  1.64s/it]  0%|          | 26/7890 [00:55<3:19:18,  1.52s/it]  0%|          | 27/7890 [00:58<3:58:04,  1.82s/it]  0%|          | 28/7890 [00:59<3:31:31,  1.61s/it]  0%|          | 29/7890 [01:00<3:19:02,  1.52s/it]  0%|          | 30/7890 [01:01<3:06:50,  1.43s/it]  0%|          | 31/7890 [01:02<2:56:10,  1.35s/it]  0%|          | 32/7890 [01:04<2:46:43,  1.27s/it]  0%|          | 33/7890 [01:05<2:53:42,  1.33s/it]  0%|          | 34/7890 [01:06<2:57:25,  1.36s/it]  0%|          | 35/7890 [01:08<2:47:15,  1.28s/it]  0%|          | 36/7890 [01:09<2:41:31,  1.23s/it]  0%|          | 37/7890 [01:10<2:49:14,  1.29s/it]  0%|          | 38/7890 [01:11<2:40:53,  1.23s/it]  0%|          | 39/7890 [01:13<2:46:40,  1.27s/it]  1%|          | 40/7890 [01:14<2:52:14,  1.32s/it]  1%|          | 41/7890 [01:16<3:11:34,  1.46s/it]  1%|          | 42/7890 [01:17<3:19:37,  1.53s/it]  1%|          | 43/7890 [01:19<3:07:07,  1.43s/it]  1%|          | 44/7890 [01:20<3:00:53,  1.38s/it]  1%|          | 45/7890 [01:21<2:49:40,  1.30s/it]  1%|          | 46/7890 [01:22<2:44:50,  1.26s/it]  1%|          | 47/7890 [01:24<2:57:47,  1.36s/it]  1%|          | 48/7890 [01:25<3:04:47,  1.41s/it]  1%|          | 49/7890 [01:28<3:56:47,  1.81s/it]  1%|          | 50/7890 [01:29<3:40:49,  1.69s/it]                                                   {'loss': 0.2076, 'grad_norm': 2.5207804104212403, 'learning_rate': 1.2658227848101267e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 105294.0, 'epoch': 0.02}
  1%|          | 50/7890 [01:29<3:40:49,  1.69s/it]  1%|          | 51/7890 [01:31<3:17:19,  1.51s/it]  1%|          | 52/7890 [01:32<3:02:04,  1.39s/it]  1%|          | 53/7890 [01:33<2:52:20,  1.32s/it]  1%|          | 54/7890 [01:35<3:31:48,  1.62s/it]  1%|          | 55/7890 [01:36<3:09:45,  1.45s/it]  1%|          | 56/7890 [01:39<3:43:57,  1.72s/it]  1%|          | 57/7890 [01:40<3:34:26,  1.64s/it]  1%|          | 58/7890 [01:42<3:27:22,  1.59s/it]  1%|          | 59/7890 [01:44<4:15:21,  1.96s/it]  1%|          | 60/7890 [01:45<3:43:27,  1.71s/it]  1%|          | 61/7890 [01:48<4:09:18,  1.91s/it]  1%|          | 62/7890 [01:49<3:53:33,  1.79s/it]  1%|          | 63/7890 [01:51<3:39:49,  1.69s/it]  1%|          | 64/7890 [01:52<3:18:32,  1.52s/it]  1%|          | 65/7890 [01:53<3:18:25,  1.52s/it]  1%|          | 66/7890 [01:55<3:15:54,  1.50s/it]  1%|          | 67/7890 [01:56<3:13:30,  1.48s/it]  1%|          | 68/7890 [01:58<3:20:28,  1.54s/it]  1%|          | 69/7890 [01:59<3:15:09,  1.50s/it]  1%|          | 70/7890 [02:02<3:47:49,  1.75s/it]  1%|          | 71/7890 [02:03<3:23:50,  1.56s/it]  1%|          | 72/7890 [02:04<3:04:53,  1.42s/it]  1%|          | 73/7890 [02:05<2:54:14,  1.34s/it]  1%|          | 74/7890 [02:06<2:44:52,  1.27s/it]  1%|          | 75/7890 [02:07<2:43:06,  1.25s/it]                                                   {'loss': 0.1721, 'grad_norm': 1.9698692046958368, 'learning_rate': 1.89873417721519e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 92221.0, 'epoch': 0.03}
  1%|          | 75/7890 [02:07<2:43:06,  1.25s/it]  1%|          | 76/7890 [02:09<2:51:45,  1.32s/it]  1%|          | 77/7890 [02:11<3:10:33,  1.46s/it]  1%|          | 78/7890 [02:12<3:18:22,  1.52s/it]  1%|          | 79/7890 [02:15<3:56:10,  1.81s/it]  1%|          | 80/7890 [02:17<3:50:12,  1.77s/it]  1%|          | 81/7890 [02:18<3:32:35,  1.63s/it]  1%|          | 82/7890 [02:19<3:19:00,  1.53s/it]  1%|          | 83/7890 [02:20<3:10:00,  1.46s/it]  1%|          | 84/7890 [02:22<3:12:02,  1.48s/it]  1%|          | 85/7890 [02:23<2:56:09,  1.35s/it]  1%|          | 86/7890 [02:24<2:45:24,  1.27s/it]  1%|          | 87/7890 [02:25<2:40:24,  1.23s/it]  1%|          | 88/7890 [02:26<2:35:16,  1.19s/it]  1%|          | 89/7890 [02:27<2:31:48,  1.17s/it]  1%|          | 90/7890 [02:29<2:34:15,  1.19s/it]  1%|          | 91/7890 [02:30<2:38:26,  1.22s/it]  1%|          | 92/7890 [02:31<2:45:41,  1.27s/it]  1%|          | 93/7890 [02:33<2:46:43,  1.28s/it]  1%|          | 94/7890 [02:34<2:40:43,  1.24s/it]  1%|          | 95/7890 [02:35<2:34:54,  1.19s/it]  1%|          | 96/7890 [02:37<2:55:40,  1.35s/it]  1%|          | 97/7890 [02:38<2:45:37,  1.28s/it]  1%|          | 98/7890 [02:39<2:49:54,  1.31s/it]  1%|▏         | 99/7890 [02:40<2:40:42,  1.24s/it]  1%|▏         | 100/7890 [02:41<2:34:51,  1.19s/it]                                                    {'loss': 0.1662, 'grad_norm': 1.8575151903396454, 'learning_rate': 1.9999661143292243e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 83530.0, 'epoch': 0.04}
  1%|▏         | 100/7890 [02:41<2:34:51,  1.19s/it]  1%|▏         | 101/7890 [02:43<2:51:35,  1.32s/it]  1%|▏         | 102/7890 [02:44<2:48:57,  1.30s/it]  1%|▏         | 103/7890 [02:45<2:50:55,  1.32s/it]  1%|▏         | 104/7890 [02:47<2:55:13,  1.35s/it]  1%|▏         | 105/7890 [02:48<2:57:29,  1.37s/it]  1%|▏         | 106/7890 [02:50<2:59:58,  1.39s/it]  1%|▏         | 107/7890 [02:51<2:48:11,  1.30s/it]  1%|▏         | 108/7890 [02:52<2:48:21,  1.30s/it]  1%|▏         | 109/7890 [02:54<2:54:45,  1.35s/it]  1%|▏         | 110/7890 [02:55<2:59:32,  1.38s/it]  1%|▏         | 111/7890 [02:56<2:56:34,  1.36s/it]  1%|▏         | 112/7890 [02:58<2:49:45,  1.31s/it]  1%|▏         | 113/7890 [02:59<2:42:47,  1.26s/it]  1%|▏         | 114/7890 [03:00<2:47:37,  1.29s/it]  1%|▏         | 115/7890 [03:02<2:55:52,  1.36s/it]  1%|▏         | 116/7890 [03:03<2:46:59,  1.29s/it]  1%|▏         | 117/7890 [03:04<2:52:50,  1.33s/it]  1%|▏         | 118/7890 [03:06<3:01:09,  1.40s/it]  2%|▏         | 119/7890 [03:07<2:49:39,  1.31s/it]  2%|▏         | 120/7890 [03:08<2:44:42,  1.27s/it]  2%|▏         | 121/7890 [03:10<2:53:54,  1.34s/it]  2%|▏         | 122/7890 [03:11<3:12:39,  1.49s/it]  2%|▏         | 123/7890 [03:12<2:58:12,  1.38s/it]  2%|▏         | 124/7890 [03:14<2:50:34,  1.32s/it]  2%|▏         | 125/7890 [03:15<2:41:08,  1.25s/it]                                                    {'loss': 0.1819, 'grad_norm': 2.00687775579909, 'learning_rate': 1.9998374139220958e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 107559.0, 'epoch': 0.05}
  2%|▏         | 125/7890 [03:15<2:41:08,  1.25s/it]  2%|▏         | 126/7890 [03:16<2:38:47,  1.23s/it]  2%|▏         | 127/7890 [03:17<2:49:42,  1.31s/it]  2%|▏         | 128/7890 [03:18<2:39:43,  1.23s/it]  2%|▏         | 129/7890 [03:20<2:36:53,  1.21s/it]  2%|▏         | 130/7890 [03:21<2:46:07,  1.28s/it]  2%|▏         | 131/7890 [03:22<2:40:45,  1.24s/it]  2%|▏         | 132/7890 [03:24<2:58:06,  1.38s/it]  2%|▏         | 133/7890 [03:25<3:05:27,  1.43s/it]  2%|▏         | 134/7890 [03:27<3:19:31,  1.54s/it]  2%|▏         | 135/7890 [03:28<3:02:04,  1.41s/it]  2%|▏         | 136/7890 [03:30<2:52:15,  1.33s/it]  2%|▏         | 137/7890 [03:31<2:43:49,  1.27s/it]  2%|▏         | 138/7890 [03:32<2:38:21,  1.23s/it]  2%|▏         | 139/7890 [03:33<2:47:12,  1.29s/it]  2%|▏         | 140/7890 [03:34<2:40:39,  1.24s/it]  2%|▏         | 141/7890 [03:36<2:50:32,  1.32s/it]  2%|▏         | 142/7890 [03:37<2:56:11,  1.36s/it]  2%|▏         | 143/7890 [03:39<2:57:54,  1.38s/it]  2%|▏         | 144/7890 [03:40<3:05:25,  1.44s/it]  2%|▏         | 145/7890 [03:42<3:11:49,  1.49s/it]  2%|▏         | 146/7890 [03:44<3:17:04,  1.53s/it]  2%|▏         | 147/7890 [03:45<2:59:50,  1.39s/it]  2%|▏         | 148/7890 [03:46<2:51:41,  1.33s/it]  2%|▏         | 149/7890 [03:47<2:53:51,  1.35s/it]  2%|▏         | 150/7890 [03:48<2:45:08,  1.28s/it]                                                    {'loss': 0.1594, 'grad_norm': 1.877361019085257, 'learning_rate': 1.9996126823714395e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 80284.0, 'epoch': 0.06}
  2%|▏         | 150/7890 [03:48<2:45:08,  1.28s/it]  2%|▏         | 151/7890 [03:50<2:45:49,  1.29s/it]  2%|▏         | 152/7890 [03:51<2:49:17,  1.31s/it]  2%|▏         | 153/7890 [03:52<2:44:11,  1.27s/it]  2%|▏         | 154/7890 [03:54<2:58:49,  1.39s/it]  2%|▏         | 155/7890 [03:55<3:09:12,  1.47s/it]  2%|▏         | 156/7890 [03:57<3:09:58,  1.47s/it]  2%|▏         | 157/7890 [03:58<2:54:01,  1.35s/it]  2%|▏         | 158/7890 [03:59<2:53:59,  1.35s/it]  2%|▏         | 159/7890 [04:00<2:44:57,  1.28s/it]  2%|▏         | 160/7890 [04:02<2:40:56,  1.25s/it]  2%|▏         | 161/7890 [04:03<2:47:58,  1.30s/it]  2%|▏         | 162/7890 [04:04<2:41:07,  1.25s/it]  2%|▏         | 163/7890 [04:05<2:37:49,  1.23s/it]  2%|▏         | 164/7890 [04:07<2:34:19,  1.20s/it]  2%|▏         | 165/7890 [04:08<2:29:12,  1.16s/it]  2%|▏         | 166/7890 [04:09<2:33:22,  1.19s/it]  2%|▏         | 167/7890 [04:11<3:13:53,  1.51s/it]  2%|▏         | 168/7890 [04:12<3:02:01,  1.41s/it]  2%|▏         | 169/7890 [04:13<2:48:47,  1.31s/it]  2%|▏         | 170/7890 [04:15<2:52:52,  1.34s/it]  2%|▏         | 171/7890 [04:16<2:43:25,  1.27s/it]  2%|▏         | 172/7890 [04:17<2:37:35,  1.23s/it]  2%|▏         | 173/7890 [04:18<2:34:18,  1.20s/it]  2%|▏         | 174/7890 [04:20<2:47:46,  1.30s/it]  2%|▏         | 175/7890 [04:21<2:55:08,  1.36s/it]                                                    {'loss': 0.1571, 'grad_norm': 2.00261029133813, 'learning_rate': 1.9992919423982265e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 98766.0, 'epoch': 0.07}
  2%|▏         | 175/7890 [04:21<2:55:08,  1.36s/it]  2%|▏         | 176/7890 [04:23<2:56:49,  1.38s/it]  2%|▏         | 177/7890 [04:24<2:57:32,  1.38s/it]  2%|▏         | 178/7890 [04:25<3:02:04,  1.42s/it]  2%|▏         | 179/7890 [04:27<3:01:04,  1.41s/it]  2%|▏         | 180/7890 [04:29<3:26:10,  1.60s/it]  2%|▏         | 181/7890 [04:30<3:14:09,  1.51s/it]  2%|▏         | 182/7890 [04:32<3:11:37,  1.49s/it]  2%|▏         | 183/7890 [04:33<3:08:45,  1.47s/it]  2%|▏         | 184/7890 [04:35<3:07:33,  1.46s/it]  2%|▏         | 185/7890 [04:36<3:09:32,  1.48s/it]  2%|▏         | 186/7890 [04:37<3:08:19,  1.47s/it]  2%|▏         | 187/7890 [04:39<3:04:41,  1.44s/it]  2%|▏         | 188/7890 [04:40<3:11:19,  1.49s/it]  2%|▏         | 189/7890 [04:42<2:57:53,  1.39s/it]  2%|▏         | 190/7890 [04:43<3:00:15,  1.40s/it]  2%|▏         | 191/7890 [04:44<2:48:21,  1.31s/it]  2%|▏         | 192/7890 [04:46<2:52:55,  1.35s/it]  2%|▏         | 193/7890 [04:47<2:44:46,  1.28s/it]  2%|▏         | 194/7890 [04:48<2:44:33,  1.28s/it]  2%|▏         | 195/7890 [04:49<2:39:28,  1.24s/it]  2%|▏         | 196/7890 [04:51<2:47:31,  1.31s/it]  2%|▏         | 197/7890 [04:52<3:03:50,  1.43s/it]  3%|▎         | 198/7890 [04:54<3:03:30,  1.43s/it]  3%|▎         | 199/7890 [04:55<2:57:19,  1.38s/it]  3%|▎         | 200/7890 [04:57<3:12:42,  1.50s/it]                                                    {'loss': 0.1609, 'grad_norm': 3.0691153252524916, 'learning_rate': 1.9988752264301398e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 94363.0, 'epoch': 0.08}
  3%|▎         | 200/7890 [04:57<3:12:42,  1.50s/it]  3%|▎         | 201/7890 [04:58<2:57:45,  1.39s/it]  3%|▎         | 202/7890 [04:59<2:52:00,  1.34s/it]  3%|▎         | 203/7890 [05:01<2:56:52,  1.38s/it]  3%|▎         | 204/7890 [05:02<2:59:04,  1.40s/it]  3%|▎         | 205/7890 [05:04<3:01:08,  1.41s/it]  3%|▎         | 206/7890 [05:05<3:01:37,  1.42s/it]  3%|▎         | 207/7890 [05:07<3:16:05,  1.53s/it]  3%|▎         | 208/7890 [05:08<3:00:25,  1.41s/it]  3%|▎         | 209/7890 [05:09<2:47:40,  1.31s/it]  3%|▎         | 210/7890 [05:10<2:46:49,  1.30s/it]  3%|▎         | 211/7890 [05:11<2:38:24,  1.24s/it]  3%|▎         | 212/7890 [05:12<2:33:45,  1.20s/it]  3%|▎         | 213/7890 [05:14<2:41:36,  1.26s/it]  3%|▎         | 214/7890 [05:16<3:11:28,  1.50s/it]  3%|▎         | 215/7890 [05:17<2:57:13,  1.39s/it]  3%|▎         | 216/7890 [05:18<2:53:10,  1.35s/it]  3%|▎         | 217/7890 [05:19<2:43:30,  1.28s/it]  3%|▎         | 218/7890 [05:20<2:35:50,  1.22s/it]  3%|▎         | 219/7890 [05:22<2:31:50,  1.19s/it]  3%|▎         | 220/7890 [05:23<2:34:54,  1.21s/it]  3%|▎         | 221/7890 [05:24<2:35:35,  1.22s/it]  3%|▎         | 222/7890 [05:25<2:39:08,  1.25s/it]  3%|▎         | 223/7890 [05:27<2:46:14,  1.30s/it]  3%|▎         | 224/7890 [05:28<2:55:08,  1.37s/it]  3%|▎         | 225/7890 [05:30<2:53:53,  1.36s/it]                                                    {'loss': 0.1567, 'grad_norm': 1.5706139270469566, 'learning_rate': 1.9983625765982958e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 86600.0, 'epoch': 0.09}
  3%|▎         | 225/7890 [05:30<2:53:53,  1.36s/it]  3%|▎         | 226/7890 [05:31<2:44:01,  1.28s/it]  3%|▎         | 227/7890 [05:32<2:56:42,  1.38s/it]  3%|▎         | 228/7890 [05:34<3:00:53,  1.42s/it]  3%|▎         | 229/7890 [05:35<2:55:40,  1.38s/it]  3%|▎         | 230/7890 [05:37<3:02:09,  1.43s/it]  3%|▎         | 231/7890 [05:38<2:48:46,  1.32s/it]  3%|▎         | 232/7890 [05:39<2:40:52,  1.26s/it]  3%|▎         | 233/7890 [05:40<2:49:08,  1.33s/it]  3%|▎         | 234/7890 [05:42<2:58:30,  1.40s/it]  3%|▎         | 235/7890 [05:43<2:47:17,  1.31s/it]  3%|▎         | 236/7890 [05:44<2:38:52,  1.25s/it]  3%|▎         | 237/7890 [05:46<2:48:55,  1.32s/it]  3%|▎         | 238/7890 [05:47<2:49:23,  1.33s/it]  3%|▎         | 239/7890 [05:48<2:52:52,  1.36s/it]  3%|▎         | 240/7890 [05:50<2:44:52,  1.29s/it]  3%|▎         | 241/7890 [05:51<2:37:53,  1.24s/it]  3%|▎         | 242/7890 [05:52<2:32:07,  1.19s/it]  3%|▎         | 243/7890 [05:53<2:31:07,  1.19s/it]  3%|▎         | 244/7890 [05:55<2:53:39,  1.36s/it]  3%|▎         | 245/7890 [05:56<2:47:38,  1.32s/it]  3%|▎         | 246/7890 [05:58<2:59:20,  1.41s/it]  3%|▎         | 247/7890 [05:59<3:04:09,  1.45s/it]  3%|▎         | 248/7890 [06:01<3:02:11,  1.43s/it]  3%|▎         | 249/7890 [06:02<3:07:22,  1.47s/it]  3%|▎         | 250/7890 [06:03<3:03:15,  1.44s/it]                                                    {'loss': 0.1535, 'grad_norm': 2.021268175146028, 'learning_rate': 1.9977540447329852e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 93062.0, 'epoch': 0.1}
  3%|▎         | 250/7890 [06:03<3:03:15,  1.44s/it]  3%|▎         | 251/7890 [06:05<3:02:14,  1.43s/it]  3%|▎         | 252/7890 [06:06<2:49:01,  1.33s/it]  3%|▎         | 253/7890 [06:07<2:46:00,  1.30s/it]  3%|▎         | 254/7890 [06:09<2:50:29,  1.34s/it]  3%|▎         | 255/7890 [06:10<2:42:12,  1.27s/it]  3%|▎         | 256/7890 [06:11<2:35:16,  1.22s/it]  3%|▎         | 257/7890 [06:12<2:35:00,  1.22s/it]  3%|▎         | 258/7890 [06:13<2:43:43,  1.29s/it]  3%|▎         | 259/7890 [06:15<2:50:32,  1.34s/it]  3%|▎         | 260/7890 [06:16<2:41:21,  1.27s/it]  3%|▎         | 261/7890 [06:17<2:47:40,  1.32s/it]  3%|▎         | 262/7890 [06:19<2:51:11,  1.35s/it]  3%|▎         | 263/7890 [06:21<3:07:08,  1.47s/it]  3%|▎         | 264/7890 [06:22<3:00:43,  1.42s/it]  3%|▎         | 265/7890 [06:23<2:56:38,  1.39s/it]  3%|▎         | 266/7890 [06:25<2:53:36,  1.37s/it]  3%|▎         | 267/7890 [06:26<2:43:17,  1.29s/it]  3%|▎         | 268/7890 [06:27<2:46:09,  1.31s/it]  3%|▎         | 269/7890 [06:28<2:49:13,  1.33s/it]  3%|▎         | 270/7890 [06:30<2:43:47,  1.29s/it]  3%|▎         | 271/7890 [06:31<2:57:16,  1.40s/it]  3%|▎         | 272/7890 [06:32<2:44:37,  1.30s/it]  3%|▎         | 273/7890 [06:34<2:47:06,  1.32s/it]  3%|▎         | 274/7890 [06:35<2:48:11,  1.32s/it]  3%|▎         | 275/7890 [06:36<2:51:09,  1.35s/it]                                                    {'loss': 0.1408, 'grad_norm': 1.4167254753622112, 'learning_rate': 1.9970496923584333e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 98618.0, 'epoch': 0.1}
  3%|▎         | 275/7890 [06:36<2:51:09,  1.35s/it]  3%|▎         | 276/7890 [06:38<2:48:50,  1.33s/it]  4%|▎         | 277/7890 [06:39<2:52:31,  1.36s/it]  4%|▎         | 278/7890 [06:40<2:50:56,  1.35s/it]  4%|▎         | 279/7890 [06:42<2:41:45,  1.28s/it]  4%|▎         | 280/7890 [06:43<2:47:37,  1.32s/it]  4%|▎         | 281/7890 [06:44<2:52:39,  1.36s/it]  4%|▎         | 282/7890 [06:46<2:45:43,  1.31s/it]  4%|▎         | 283/7890 [06:47<2:37:20,  1.24s/it]  4%|▎         | 284/7890 [06:48<2:46:51,  1.32s/it]  4%|▎         | 285/7890 [06:50<2:49:57,  1.34s/it]  4%|▎         | 286/7890 [06:51<2:42:32,  1.28s/it]  4%|▎         | 287/7890 [06:52<2:45:41,  1.31s/it]  4%|▎         | 288/7890 [06:54<2:48:04,  1.33s/it]  4%|▎         | 289/7890 [06:55<2:38:17,  1.25s/it]  4%|▎         | 290/7890 [06:56<2:32:21,  1.20s/it]  4%|▎         | 291/7890 [06:57<2:31:30,  1.20s/it]  4%|▎         | 292/7890 [06:58<2:31:36,  1.20s/it]  4%|▎         | 293/7890 [06:59<2:28:54,  1.18s/it]  4%|▎         | 294/7890 [07:01<2:37:48,  1.25s/it]  4%|▎         | 295/7890 [07:02<2:46:54,  1.32s/it]  4%|▍         | 296/7890 [07:03<2:43:42,  1.29s/it]  4%|▍         | 297/7890 [07:04<2:38:48,  1.25s/it]  4%|▍         | 298/7890 [07:06<2:58:31,  1.41s/it]  4%|▍         | 299/7890 [07:08<2:55:45,  1.39s/it]  4%|▍         | 300/7890 [07:09<2:45:35,  1.31s/it]                                                    {'loss': 0.1429, 'grad_norm': 1.7558515673708852, 'learning_rate': 1.9962495906865793e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 82662.0, 'epoch': 0.11}
  4%|▍         | 300/7890 [07:09<2:45:35,  1.31s/it]  4%|▍         | 301/7890 [07:10<2:50:37,  1.35s/it]  4%|▍         | 302/7890 [07:11<2:38:28,  1.25s/it]  4%|▍         | 303/7890 [07:12<2:36:42,  1.24s/it]  4%|▍         | 304/7890 [07:14<2:31:33,  1.20s/it]  4%|▍         | 305/7890 [07:15<2:28:09,  1.17s/it]  4%|▍         | 306/7890 [07:16<2:26:47,  1.16s/it]  4%|▍         | 307/7890 [07:17<2:35:16,  1.23s/it]  4%|▍         | 308/7890 [07:19<2:47:46,  1.33s/it]  4%|▍         | 309/7890 [07:20<2:44:23,  1.30s/it]  4%|▍         | 310/7890 [07:21<2:39:05,  1.26s/it]  4%|▍         | 311/7890 [07:22<2:40:34,  1.27s/it]  4%|▍         | 312/7890 [07:24<2:57:38,  1.41s/it]  4%|▍         | 313/7890 [07:25<2:55:18,  1.39s/it]  4%|▍         | 314/7890 [07:27<3:00:10,  1.43s/it]  4%|▍         | 315/7890 [07:28<2:47:00,  1.32s/it]  4%|▍         | 316/7890 [07:29<2:50:13,  1.35s/it]  4%|▍         | 317/7890 [07:31<2:54:34,  1.38s/it]  4%|▍         | 318/7890 [07:32<2:47:55,  1.33s/it]  4%|▍         | 319/7890 [07:33<2:41:29,  1.28s/it]  4%|▍         | 320/7890 [07:35<2:41:39,  1.28s/it]  4%|▍         | 321/7890 [07:36<2:47:31,  1.33s/it]  4%|▍         | 322/7890 [07:37<2:52:43,  1.37s/it]  4%|▍         | 323/7890 [07:39<2:56:11,  1.40s/it]  4%|▍         | 324/7890 [07:40<2:44:55,  1.31s/it]  4%|▍         | 325/7890 [07:41<2:49:09,  1.34s/it]                                                    {'loss': 0.1505, 'grad_norm': 1.829175793821233, 'learning_rate': 1.9953538206098768e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 85928.0, 'epoch': 0.12}
  4%|▍         | 325/7890 [07:41<2:49:09,  1.34s/it]  4%|▍         | 326/7890 [07:43<2:44:04,  1.30s/it]  4%|▍         | 327/7890 [07:44<2:49:36,  1.35s/it]  4%|▍         | 328/7890 [07:45<2:49:50,  1.35s/it]  4%|▍         | 329/7890 [07:47<3:02:14,  1.45s/it]  4%|▍         | 330/7890 [07:48<2:50:40,  1.35s/it]  4%|▍         | 331/7890 [07:50<2:52:01,  1.37s/it]  4%|▍         | 332/7890 [07:51<2:43:57,  1.30s/it]  4%|▍         | 333/7890 [07:52<2:36:11,  1.24s/it]  4%|▍         | 334/7890 [07:53<2:47:04,  1.33s/it]  4%|▍         | 335/7890 [07:55<2:43:16,  1.30s/it]  4%|▍         | 336/7890 [07:56<2:34:31,  1.23s/it]  4%|▍         | 337/7890 [07:57<2:38:45,  1.26s/it]  4%|▍         | 338/7890 [07:58<2:40:56,  1.28s/it]  4%|▍         | 339/7890 [08:00<2:35:50,  1.24s/it]  4%|▍         | 340/7890 [08:01<2:39:30,  1.27s/it]  4%|▍         | 341/7890 [08:02<2:32:59,  1.22s/it]  4%|▍         | 342/7890 [08:03<2:33:20,  1.22s/it]  4%|▍         | 343/7890 [08:04<2:30:09,  1.19s/it]  4%|▍         | 344/7890 [08:06<2:38:59,  1.26s/it]  4%|▍         | 345/7890 [08:07<2:34:39,  1.23s/it]  4%|▍         | 346/7890 [08:08<2:29:36,  1.19s/it]  4%|▍         | 347/7890 [08:09<2:32:15,  1.21s/it]  4%|▍         | 348/7890 [08:10<2:29:04,  1.19s/it]  4%|▍         | 349/7890 [08:12<2:38:04,  1.26s/it]  4%|▍         | 350/7890 [08:13<2:47:28,  1.33s/it]                                                    {'loss': 0.1408, 'grad_norm': 1.7227403961874959, 'learning_rate': 1.9943624726931135e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 70868.0, 'epoch': 0.13}
  4%|▍         | 350/7890 [08:13<2:47:28,  1.33s/it]  4%|▍         | 351/7890 [08:15<3:16:28,  1.56s/it]  4%|▍         | 352/7890 [08:17<2:58:18,  1.42s/it]  4%|▍         | 353/7890 [08:18<2:58:59,  1.42s/it]  4%|▍         | 354/7890 [08:19<2:53:26,  1.38s/it]  4%|▍         | 355/7890 [08:20<2:42:51,  1.30s/it]  5%|▍         | 356/7890 [08:22<2:46:22,  1.32s/it]  5%|▍         | 357/7890 [08:23<2:45:30,  1.32s/it]  5%|▍         | 358/7890 [08:25<2:52:03,  1.37s/it]  5%|▍         | 359/7890 [08:26<2:56:21,  1.40s/it]  5%|▍         | 360/7890 [08:28<3:00:08,  1.44s/it]  5%|▍         | 361/7890 [08:29<2:57:21,  1.41s/it]  5%|▍         | 362/7890 [08:31<3:23:29,  1.62s/it]  5%|▍         | 363/7890 [08:32<3:05:41,  1.48s/it]  5%|▍         | 364/7890 [08:33<2:50:33,  1.36s/it]  5%|▍         | 365/7890 [08:34<2:40:14,  1.28s/it]  5%|▍         | 366/7890 [08:36<2:46:34,  1.33s/it]  5%|▍         | 367/7890 [08:37<2:41:26,  1.29s/it]  5%|▍         | 368/7890 [08:38<2:46:40,  1.33s/it]  5%|▍         | 369/7890 [08:40<2:47:56,  1.34s/it]  5%|▍         | 370/7890 [08:41<2:38:47,  1.27s/it]  5%|▍         | 371/7890 [08:42<2:41:53,  1.29s/it]  5%|▍         | 372/7890 [08:44<2:51:54,  1.37s/it]  5%|▍         | 373/7890 [08:45<2:45:24,  1.32s/it]  5%|▍         | 374/7890 [08:46<2:38:34,  1.27s/it]  5%|▍         | 375/7890 [08:47<2:33:54,  1.23s/it]                                                    {'loss': 0.1351, 'grad_norm': 1.5583072867571168, 'learning_rate': 1.9932756471642585e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 91460.0, 'epoch': 0.14}
  5%|▍         | 375/7890 [08:47<2:33:54,  1.23s/it]  5%|▍         | 376/7890 [08:48<2:28:50,  1.19s/it]  5%|▍         | 377/7890 [08:49<2:25:50,  1.16s/it]  5%|▍         | 378/7890 [08:51<2:31:59,  1.21s/it]  5%|▍         | 379/7890 [08:52<2:45:07,  1.32s/it]  5%|▍         | 380/7890 [08:54<2:56:51,  1.41s/it]  5%|▍         | 381/7890 [08:55<2:49:39,  1.36s/it]  5%|▍         | 382/7890 [08:56<2:39:54,  1.28s/it]  5%|▍         | 383/7890 [08:58<3:02:58,  1.46s/it]  5%|▍         | 384/7890 [09:00<3:05:43,  1.48s/it]  5%|▍         | 385/7890 [09:01<2:55:39,  1.40s/it]  5%|▍         | 386/7890 [09:02<2:57:02,  1.42s/it]  5%|▍         | 387/7890 [09:04<2:46:50,  1.33s/it]  5%|▍         | 388/7890 [09:05<2:51:41,  1.37s/it]  5%|▍         | 389/7890 [09:07<3:08:27,  1.51s/it]  5%|▍         | 390/7890 [09:09<3:23:15,  1.63s/it]  5%|▍         | 391/7890 [09:10<3:11:24,  1.53s/it]  5%|▍         | 392/7890 [09:11<3:05:11,  1.48s/it]  5%|▍         | 393/7890 [09:13<2:56:40,  1.41s/it]  5%|▍         | 394/7890 [09:14<3:03:13,  1.47s/it]  5%|▌         | 395/7890 [09:16<3:03:29,  1.47s/it]  5%|▌         | 396/7890 [09:17<3:00:36,  1.45s/it]  5%|▌         | 397/7890 [09:19<3:12:23,  1.54s/it]  5%|▌         | 398/7890 [09:20<2:55:50,  1.41s/it]  5%|▌         | 399/7890 [09:21<2:43:01,  1.31s/it]  5%|▌         | 400/7890 [09:22<2:35:47,  1.25s/it]                                                    {'loss': 0.1437, 'grad_norm': 1.574459313122244, 'learning_rate': 1.9920934539043254e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 67054.0, 'epoch': 0.15}
  5%|▌         | 400/7890 [09:22<2:35:47,  1.25s/it]  5%|▌         | 401/7890 [09:24<2:41:34,  1.29s/it]  5%|▌         | 402/7890 [09:25<2:44:49,  1.32s/it]  5%|▌         | 403/7890 [09:27<2:58:11,  1.43s/it]  5%|▌         | 404/7890 [09:28<2:51:42,  1.38s/it]  5%|▌         | 405/7890 [09:29<2:39:53,  1.28s/it]  5%|▌         | 406/7890 [09:30<2:32:58,  1.23s/it]  5%|▌         | 407/7890 [09:31<2:28:54,  1.19s/it]  5%|▌         | 408/7890 [09:32<2:28:41,  1.19s/it]  5%|▌         | 409/7890 [09:34<2:39:00,  1.28s/it]  5%|▌         | 410/7890 [09:35<2:54:28,  1.40s/it]  5%|▌         | 411/7890 [09:37<2:42:24,  1.30s/it]  5%|▌         | 412/7890 [09:38<2:34:51,  1.24s/it]  5%|▌         | 413/7890 [09:39<2:30:37,  1.21s/it]  5%|▌         | 414/7890 [09:40<2:31:42,  1.22s/it]  5%|▌         | 415/7890 [09:41<2:28:34,  1.19s/it]  5%|▌         | 416/7890 [09:43<2:36:45,  1.26s/it]  5%|▌         | 417/7890 [09:45<3:05:02,  1.49s/it]  5%|▌         | 418/7890 [09:46<3:05:13,  1.49s/it]  5%|▌         | 419/7890 [09:47<2:50:46,  1.37s/it]  5%|▌         | 420/7890 [09:48<2:43:05,  1.31s/it]  5%|▌         | 421/7890 [09:50<2:48:41,  1.36s/it]  5%|▌         | 422/7890 [09:51<2:38:44,  1.28s/it]  5%|▌         | 423/7890 [09:52<2:45:04,  1.33s/it]  5%|▌         | 424/7890 [09:53<2:35:28,  1.25s/it]  5%|▌         | 425/7890 [09:55<2:50:07,  1.37s/it]                                                    {'loss': 0.1457, 'grad_norm': 1.3529194981943162, 'learning_rate': 1.9908160124362655e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 69870.0, 'epoch': 0.16}
  5%|▌         | 425/7890 [09:55<2:50:07,  1.37s/it]  5%|▌         | 426/7890 [09:57<3:02:38,  1.47s/it]  5%|▌         | 427/7890 [09:59<3:14:19,  1.56s/it]  5%|▌         | 428/7890 [10:00<3:06:21,  1.50s/it]  5%|▌         | 429/7890 [10:01<2:56:19,  1.42s/it]  5%|▌         | 430/7890 [10:02<2:50:14,  1.37s/it]  5%|▌         | 431/7890 [10:04<2:45:08,  1.33s/it]  5%|▌         | 432/7890 [10:05<2:49:15,  1.36s/it]  5%|▌         | 433/7890 [10:06<2:40:05,  1.29s/it]  6%|▌         | 434/7890 [10:08<2:42:25,  1.31s/it]  6%|▌         | 435/7890 [10:09<2:53:23,  1.40s/it]  6%|▌         | 436/7890 [10:10<2:51:59,  1.38s/it]  6%|▌         | 437/7890 [10:12<3:03:51,  1.48s/it]  6%|▌         | 438/7890 [10:14<3:02:09,  1.47s/it]  6%|▌         | 439/7890 [10:15<2:48:39,  1.36s/it]  6%|▌         | 440/7890 [10:16<2:46:23,  1.34s/it]  6%|▌         | 441/7890 [10:17<2:43:51,  1.32s/it]  6%|▌         | 442/7890 [10:19<2:43:23,  1.32s/it]  6%|▌         | 443/7890 [10:20<2:44:22,  1.32s/it]  6%|▌         | 444/7890 [10:21<2:35:48,  1.26s/it]  6%|▌         | 445/7890 [10:22<2:30:16,  1.21s/it]  6%|▌         | 446/7890 [10:23<2:26:37,  1.18s/it]  6%|▌         | 447/7890 [10:24<2:24:16,  1.16s/it]  6%|▌         | 448/7890 [10:26<2:27:52,  1.19s/it]  6%|▌         | 449/7890 [10:27<2:34:06,  1.24s/it]  6%|▌         | 450/7890 [10:29<2:49:45,  1.37s/it]                                                    {'loss': 0.1383, 'grad_norm': 1.6810802274731482, 'learning_rate': 1.9894434519128824e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 78566.0, 'epoch': 0.17}
  6%|▌         | 450/7890 [10:29<2:49:45,  1.37s/it]  6%|▌         | 451/7890 [10:30<2:58:39,  1.44s/it]  6%|▌         | 452/7890 [10:32<3:08:04,  1.52s/it]  6%|▌         | 453/7890 [10:33<3:00:22,  1.46s/it]  6%|▌         | 454/7890 [10:34<2:50:39,  1.38s/it]  6%|▌         | 455/7890 [10:36<2:51:12,  1.38s/it]  6%|▌         | 456/7890 [10:37<2:53:12,  1.40s/it]  6%|▌         | 457/7890 [10:38<2:42:52,  1.31s/it]  6%|▌         | 458/7890 [10:40<2:52:39,  1.39s/it]  6%|▌         | 459/7890 [10:41<2:51:30,  1.38s/it]  6%|▌         | 460/7890 [10:43<2:48:46,  1.36s/it]  6%|▌         | 461/7890 [10:44<3:02:10,  1.47s/it]  6%|▌         | 462/7890 [10:46<2:51:20,  1.38s/it]  6%|▌         | 463/7890 [10:47<2:54:03,  1.41s/it]  6%|▌         | 464/7890 [10:49<2:56:56,  1.43s/it]  6%|▌         | 465/7890 [10:50<2:54:25,  1.41s/it]  6%|▌         | 466/7890 [10:51<2:52:10,  1.39s/it]  6%|▌         | 467/7890 [10:52<2:42:17,  1.31s/it]  6%|▌         | 468/7890 [10:54<2:46:48,  1.35s/it]  6%|▌         | 469/7890 [10:55<2:49:03,  1.37s/it]  6%|▌         | 470/7890 [10:56<2:42:21,  1.31s/it]  6%|▌         | 471/7890 [10:58<2:40:52,  1.30s/it]  6%|▌         | 472/7890 [10:59<2:34:41,  1.25s/it]  6%|▌         | 473/7890 [11:00<2:37:08,  1.27s/it]  6%|▌         | 474/7890 [11:01<2:41:16,  1.30s/it]  6%|▌         | 475/7890 [11:03<2:36:44,  1.27s/it]                                                    {'loss': 0.1604, 'grad_norm': 1.449788272132121, 'learning_rate': 1.9879759111037746e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 88654.0, 'epoch': 0.18}
  6%|▌         | 475/7890 [11:03<2:36:44,  1.27s/it]  6%|▌         | 476/7890 [11:04<2:36:15,  1.26s/it]  6%|▌         | 477/7890 [11:05<2:28:50,  1.20s/it]  6%|▌         | 478/7890 [11:06<2:31:56,  1.23s/it]  6%|▌         | 479/7890 [11:07<2:26:08,  1.18s/it]  6%|▌         | 480/7890 [11:08<2:22:59,  1.16s/it]  6%|▌         | 481/7890 [11:10<2:37:01,  1.27s/it]  6%|▌         | 482/7890 [11:11<2:37:26,  1.28s/it]  6%|▌         | 483/7890 [11:12<2:30:14,  1.22s/it]  6%|▌         | 484/7890 [11:13<2:25:52,  1.18s/it]  6%|▌         | 485/7890 [11:15<2:30:32,  1.22s/it]  6%|▌         | 486/7890 [11:16<2:47:59,  1.36s/it]  6%|▌         | 487/7890 [11:18<2:41:33,  1.31s/it]  6%|▌         | 488/7890 [11:19<2:33:57,  1.25s/it]  6%|▌         | 489/7890 [11:20<2:27:31,  1.20s/it]  6%|▌         | 490/7890 [11:21<2:29:45,  1.21s/it]  6%|▌         | 491/7890 [11:23<2:46:58,  1.35s/it]  6%|▌         | 492/7890 [11:24<2:49:06,  1.37s/it]  6%|▌         | 493/7890 [11:26<2:57:49,  1.44s/it]  6%|▋         | 494/7890 [11:27<2:56:58,  1.44s/it]  6%|▋         | 495/7890 [11:29<2:55:01,  1.42s/it]  6%|▋         | 496/7890 [11:30<2:49:02,  1.37s/it]  6%|▋         | 497/7890 [11:32<3:04:16,  1.50s/it]  6%|▋         | 498/7890 [11:33<2:52:01,  1.40s/it]  6%|▋         | 499/7890 [11:34<2:51:33,  1.39s/it]  6%|▋         | 500/7890 [11:36<2:50:17,  1.38s/it]                                                    {'loss': 0.1368, 'grad_norm': 1.325210335069374, 'learning_rate': 1.9864135383813052e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 106493.0, 'epoch': 0.19}
  6%|▋         | 500/7890 [11:36<2:50:17,  1.38s/it][2025-08-27 22:10:50,773] [INFO] [mcore_adapter.models.model_factory]: Saving model checkpoint to saves/qwen3-1.7B-tp_dp/lora/sft/checkpoint-500/iter_0000001/mp_rank_01/model_optim_rng.pt
[INFO     | mcore_adapter.models.model_factory]: Saving model checkpoint to saves/qwen3-1.7B-tp_dp/lora/sft/checkpoint-500/iter_0000001/mp_rank_01/model_optim_rng.pt
[2025-08-27 22:10:50,961] [INFO] [mcore_adapter.models.model_factory]: Saving model checkpoint to saves/qwen3-1.7B-tp_dp/lora/sft/checkpoint-500/iter_0000001/mp_rank_00/model_optim_rng.pt
[INFO     | mcore_adapter.models.model_factory]: Saving model checkpoint to saves/qwen3-1.7B-tp_dp/lora/sft/checkpoint-500/iter_0000001/mp_rank_00/model_optim_rng.pt
[WARNING|2025-08-27 22:10:55] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-08-27 22:10:56,428] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 22:10:56,672] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 22:10:56,793] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-27 22:10:57,555] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
/home/qianwenhao/.conda/envs/llamafactory/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm
  warnings.warn('Apex is not installed. Falling back to Torch Norm')
  6%|▋         | 501/7890 [13:30<72:11:23, 35.17s/it]  6%|▋         | 502/7890 [13:31<51:15:25, 24.98s/it]  6%|▋         | 503/7890 [13:32<36:39:06, 17.86s/it]  6%|▋         | 504/7890 [13:33<26:27:48, 12.90s/it]  6%|▋         | 505/7890 [13:35<19:23:52,  9.46s/it]  6%|▋         | 506/7890 [13:37<14:42:00,  7.17s/it]  6%|▋         | 507/7890 [13:38<11:01:33,  5.38s/it]  6%|▋         | 508/7890 [13:39<8:23:26,  4.09s/it]   6%|▋         | 509/7890 [13:40<6:47:49,  3.32s/it]  6%|▋         | 510/7890 [13:41<5:25:55,  2.65s/it]  6%|▋         | 511/7890 [13:43<4:41:17,  2.29s/it]  6%|▋         | 512/7890 [13:44<4:12:35,  2.05s/it]  7%|▋         | 513/7890 [13:46<3:49:18,  1.87s/it]  7%|▋         | 514/7890 [13:47<3:32:48,  1.73s/it]  7%|▋         | 515/7890 [13:49<3:23:35,  1.66s/it]  7%|▋         | 516/7890 [13:50<3:11:19,  1.56s/it]  7%|▋         | 517/7890 [13:51<2:58:58,  1.46s/it]  7%|▋         | 518/7890 [13:53<3:04:41,  1.50s/it]  7%|▋         | 519/7890 [13:54<3:03:44,  1.50s/it]  7%|▋         | 520/7890 [13:56<2:59:45,  1.46s/it]  7%|▋         | 521/7890 [13:57<3:04:16,  1.50s/it]  7%|▋         | 522/7890 [13:58<2:51:21,  1.40s/it]  7%|▋         | 523/7890 [14:00<2:44:37,  1.34s/it]  7%|▋         | 524/7890 [14:01<2:52:10,  1.40s/it]  7%|▋         | 525/7890 [14:03<2:53:35,  1.41s/it]                                                    {'loss': 0.1287, 'grad_norm': 1.54896773758962, 'learning_rate': 1.9847564917056024e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 89274.0, 'epoch': 0.2}
  7%|▋         | 525/7890 [14:03<2:53:35,  1.41s/it]  7%|▋         | 526/7890 [14:04<2:41:54,  1.32s/it]  7%|▋         | 527/7890 [14:05<2:33:18,  1.25s/it]  7%|▋         | 528/7890 [14:06<2:32:00,  1.24s/it]  7%|▋         | 529/7890 [14:07<2:29:13,  1.22s/it]  7%|▋         | 530/7890 [14:08<2:25:45,  1.19s/it]  7%|▋         | 531/7890 [14:10<2:30:20,  1.23s/it]  7%|▋         | 532/7890 [14:12<2:52:17,  1.40s/it]  7%|▋         | 533/7890 [14:13<2:43:13,  1.33s/it]  7%|▋         | 534/7890 [14:14<2:47:16,  1.36s/it]  7%|▋         | 535/7890 [14:15<2:39:09,  1.30s/it]  7%|▋         | 536/7890 [14:17<2:45:52,  1.35s/it]  7%|▋         | 537/7890 [14:18<2:39:03,  1.30s/it]  7%|▋         | 538/7890 [14:19<2:33:36,  1.25s/it]  7%|▋         | 539/7890 [14:21<2:43:17,  1.33s/it]  7%|▋         | 540/7890 [14:22<2:46:56,  1.36s/it]  7%|▋         | 541/7890 [14:23<2:37:24,  1.29s/it]  7%|▋         | 542/7890 [14:25<2:41:56,  1.32s/it]  7%|▋         | 543/7890 [14:26<2:46:05,  1.36s/it]  7%|▋         | 544/7890 [14:27<2:47:59,  1.37s/it]  7%|▋         | 545/7890 [14:29<2:50:25,  1.39s/it]  7%|▋         | 546/7890 [14:31<3:05:43,  1.52s/it]  7%|▋         | 547/7890 [14:32<2:59:47,  1.47s/it]  7%|▋         | 548/7890 [14:33<2:45:12,  1.35s/it]  7%|▋         | 549/7890 [14:34<2:47:46,  1.37s/it]  7%|▋         | 550/7890 [14:36<2:50:05,  1.39s/it]                                                    {'loss': 0.1311, 'grad_norm': 1.1224494207324716, 'learning_rate': 1.9830049386085877e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 82772.0, 'epoch': 0.21}
  7%|▋         | 550/7890 [14:36<2:50:05,  1.39s/it]  7%|▋         | 551/7890 [14:37<2:49:48,  1.39s/it]  7%|▋         | 552/7890 [14:39<2:56:59,  1.45s/it]  7%|▋         | 553/7890 [14:40<2:59:30,  1.47s/it]  7%|▋         | 554/7890 [14:42<3:07:45,  1.54s/it]  7%|▋         | 555/7890 [14:43<2:52:15,  1.41s/it]  7%|▋         | 556/7890 [14:45<2:54:32,  1.43s/it]  7%|▋         | 557/7890 [14:46<3:05:03,  1.51s/it]  7%|▋         | 558/7890 [14:48<2:56:00,  1.44s/it]  7%|▋         | 559/7890 [14:49<2:53:29,  1.42s/it]  7%|▋         | 560/7890 [14:51<2:56:28,  1.44s/it]  7%|▋         | 561/7890 [14:52<2:47:15,  1.37s/it]  7%|▋         | 562/7890 [14:53<2:50:38,  1.40s/it]  7%|▋         | 563/7890 [14:55<2:56:38,  1.45s/it]  7%|▋         | 564/7890 [14:56<3:00:03,  1.47s/it]  7%|▋         | 565/7890 [14:57<2:47:21,  1.37s/it]  7%|▋         | 566/7890 [14:59<2:44:13,  1.35s/it]  7%|▋         | 567/7890 [15:00<2:47:39,  1.37s/it]  7%|▋         | 568/7890 [15:02<3:03:55,  1.51s/it]  7%|▋         | 569/7890 [15:03<2:57:55,  1.46s/it]  7%|▋         | 570/7890 [15:05<2:48:33,  1.38s/it]  7%|▋         | 571/7890 [15:06<2:51:37,  1.41s/it]  7%|▋         | 572/7890 [15:07<2:40:13,  1.31s/it]  7%|▋         | 573/7890 [15:08<2:37:43,  1.29s/it]  7%|▋         | 574/7890 [15:09<2:29:59,  1.23s/it]  7%|▋         | 575/7890 [15:11<2:30:05,  1.23s/it]                                                    {'loss': 0.1402, 'grad_norm': 1.4298012276530414, 'learning_rate': 1.981159056177038e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 69809.0, 'epoch': 0.22}
  7%|▋         | 575/7890 [15:11<2:30:05,  1.23s/it]  7%|▋         | 576/7890 [15:12<2:38:10,  1.30s/it]  7%|▋         | 577/7890 [15:13<2:35:46,  1.28s/it]  7%|▋         | 578/7890 [15:15<2:44:20,  1.35s/it]  7%|▋         | 579/7890 [15:16<2:44:51,  1.35s/it]  7%|▋         | 580/7890 [15:17<2:38:06,  1.30s/it]  7%|▋         | 581/7890 [15:19<2:43:22,  1.34s/it]  7%|▋         | 582/7890 [15:21<2:57:25,  1.46s/it]  7%|▋         | 583/7890 [15:22<2:59:12,  1.47s/it]  7%|▋         | 584/7890 [15:24<3:00:52,  1.49s/it]  7%|▋         | 585/7890 [15:25<3:16:15,  1.61s/it]  7%|▋         | 586/7890 [15:27<3:07:01,  1.54s/it]  7%|▋         | 587/7890 [15:28<3:01:28,  1.49s/it]  7%|▋         | 588/7890 [15:29<2:50:13,  1.40s/it]  7%|▋         | 589/7890 [15:31<2:47:28,  1.38s/it]  7%|▋         | 590/7890 [15:32<2:46:56,  1.37s/it]  7%|▋         | 591/7890 [15:33<2:38:49,  1.31s/it]  8%|▊         | 592/7890 [15:35<2:45:32,  1.36s/it]  8%|▊         | 593/7890 [15:36<2:47:17,  1.38s/it]  8%|▊         | 594/7890 [15:38<2:48:29,  1.39s/it]  8%|▊         | 595/7890 [15:39<2:50:31,  1.40s/it]  8%|▊         | 596/7890 [15:40<2:41:00,  1.32s/it]  8%|▊         | 597/7890 [15:42<2:44:28,  1.35s/it]  8%|▊         | 598/7890 [15:43<2:58:46,  1.47s/it]  8%|▊         | 599/7890 [15:45<2:57:27,  1.46s/it]  8%|▊         | 600/7890 [15:46<2:42:48,  1.34s/it]                                                    {'loss': 0.1371, 'grad_norm': 1.4604009775288553, 'learning_rate': 1.979219031034684e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 102441.0, 'epoch': 0.23}
  8%|▊         | 600/7890 [15:46<2:42:48,  1.34s/it]  8%|▊         | 601/7890 [15:47<2:36:01,  1.28s/it]  8%|▊         | 602/7890 [15:48<2:38:15,  1.30s/it]  8%|▊         | 603/7890 [15:49<2:32:59,  1.26s/it]  8%|▊         | 604/7890 [15:51<2:29:28,  1.23s/it]  8%|▊         | 605/7890 [15:52<2:26:14,  1.20s/it]  8%|▊         | 606/7890 [15:53<2:22:03,  1.17s/it]  8%|▊         | 607/7890 [15:54<2:25:53,  1.20s/it]  8%|▊         | 608/7890 [15:56<2:37:03,  1.29s/it]  8%|▊         | 609/7890 [15:57<2:42:37,  1.34s/it]  8%|▊         | 610/7890 [15:59<2:55:48,  1.45s/it]  8%|▊         | 611/7890 [16:00<2:45:12,  1.36s/it]  8%|▊         | 612/7890 [16:01<2:39:25,  1.31s/it]  8%|▊         | 613/7890 [16:03<2:43:47,  1.35s/it]  8%|▊         | 614/7890 [16:04<2:46:21,  1.37s/it]  8%|▊         | 615/7890 [16:06<3:01:12,  1.49s/it]  8%|▊         | 616/7890 [16:08<3:11:45,  1.58s/it]  8%|▊         | 617/7890 [16:09<3:05:50,  1.53s/it]  8%|▊         | 618/7890 [16:10<3:05:31,  1.53s/it]  8%|▊         | 619/7890 [16:12<2:56:58,  1.46s/it]  8%|▊         | 620/7890 [16:13<2:47:38,  1.38s/it]  8%|▊         | 621/7890 [16:14<2:48:43,  1.39s/it]  8%|▊         | 622/7890 [16:16<2:50:47,  1.41s/it]  8%|▊         | 623/7890 [16:17<2:41:26,  1.33s/it]  8%|▊         | 624/7890 [16:19<2:57:56,  1.47s/it]  8%|▊         | 625/7890 [16:20<2:55:13,  1.45s/it]                                                    {'loss': 0.1383, 'grad_norm': 1.3240670986604364, 'learning_rate': 1.9771850593233385e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 102892.0, 'epoch': 0.24}
  8%|▊         | 625/7890 [16:20<2:55:13,  1.45s/it]  8%|▊         | 626/7890 [16:22<2:50:08,  1.41s/it]  8%|▊         | 627/7890 [16:23<2:57:45,  1.47s/it]  8%|▊         | 628/7890 [16:24<2:54:10,  1.44s/it]  8%|▊         | 629/7890 [16:26<2:44:31,  1.36s/it]  8%|▊         | 630/7890 [16:27<2:39:00,  1.31s/it]  8%|▊         | 631/7890 [16:29<2:55:39,  1.45s/it]  8%|▊         | 632/7890 [16:30<2:55:11,  1.45s/it]  8%|▊         | 633/7890 [16:31<2:43:57,  1.36s/it]  8%|▊         | 634/7890 [16:32<2:34:12,  1.28s/it]  8%|▊         | 635/7890 [16:33<2:26:46,  1.21s/it]  8%|▊         | 636/7890 [16:35<2:34:12,  1.28s/it]  8%|▊         | 637/7890 [16:36<2:27:07,  1.22s/it]  8%|▊         | 638/7890 [16:37<2:23:51,  1.19s/it]  8%|▊         | 639/7890 [16:38<2:29:52,  1.24s/it]  8%|▊         | 640/7890 [16:39<2:24:15,  1.19s/it]  8%|▊         | 641/7890 [16:41<2:46:58,  1.38s/it]  8%|▊         | 642/7890 [16:43<3:02:48,  1.51s/it]  8%|▊         | 643/7890 [16:44<2:48:15,  1.39s/it]  8%|▊         | 644/7890 [16:45<2:41:19,  1.34s/it]  8%|▊         | 645/7890 [16:47<2:44:40,  1.36s/it]  8%|▊         | 646/7890 [16:48<2:38:47,  1.32s/it]  8%|▊         | 647/7890 [16:49<2:38:24,  1.31s/it]  8%|▊         | 648/7890 [16:50<2:30:24,  1.25s/it]  8%|▊         | 649/7890 [16:52<2:26:26,  1.21s/it]  8%|▊         | 650/7890 [16:53<2:21:54,  1.18s/it]                                                    {'loss': 0.1181, 'grad_norm': 1.1800168821775903, 'learning_rate': 1.9750573466830686e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 83150.0, 'epoch': 0.25}
  8%|▊         | 650/7890 [16:53<2:21:54,  1.18s/it]  8%|▊         | 651/7890 [16:54<2:31:24,  1.25s/it]  8%|▊         | 652/7890 [16:55<2:31:37,  1.26s/it]  8%|▊         | 653/7890 [16:57<2:31:20,  1.25s/it]  8%|▊         | 654/7890 [16:58<2:24:04,  1.19s/it]  8%|▊         | 655/7890 [16:59<2:22:56,  1.19s/it]  8%|▊         | 656/7890 [17:00<2:28:45,  1.23s/it]  8%|▊         | 657/7890 [17:01<2:26:38,  1.22s/it]  8%|▊         | 658/7890 [17:03<2:24:28,  1.20s/it]  8%|▊         | 659/7890 [17:04<2:22:36,  1.18s/it]  8%|▊         | 660/7890 [17:05<2:32:00,  1.26s/it]  8%|▊         | 661/7890 [17:06<2:25:29,  1.21s/it]  8%|▊         | 662/7890 [17:07<2:24:24,  1.20s/it]  8%|▊         | 663/7890 [17:08<2:19:30,  1.16s/it]  8%|▊         | 664/7890 [17:10<2:20:11,  1.16s/it]  8%|▊         | 665/7890 [17:11<2:41:50,  1.34s/it]  8%|▊         | 666/7890 [17:13<2:48:38,  1.40s/it]  8%|▊         | 667/7890 [17:14<2:53:12,  1.44s/it]  8%|▊         | 668/7890 [17:16<2:46:30,  1.38s/it]  8%|▊         | 669/7890 [17:17<2:35:54,  1.30s/it]  8%|▊         | 670/7890 [17:18<2:39:05,  1.32s/it]  9%|▊         | 671/7890 [17:20<2:53:44,  1.44s/it]  9%|▊         | 672/7890 [17:21<2:51:57,  1.43s/it]  9%|▊         | 673/7890 [17:23<2:46:03,  1.38s/it]  9%|▊         | 674/7890 [17:24<2:48:20,  1.40s/it]  9%|▊         | 675/7890 [17:25<2:38:53,  1.32s/it]                                                    {'loss': 0.1213, 'grad_norm': 0.9390394287321934, 'learning_rate': 1.9728361082314037e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 95329.0, 'epoch': 0.26}
  9%|▊         | 675/7890 [17:25<2:38:53,  1.32s/it]  9%|▊         | 676/7890 [17:27<2:43:54,  1.36s/it]  9%|▊         | 677/7890 [17:28<2:41:20,  1.34s/it]  9%|▊         | 678/7890 [17:29<2:47:06,  1.39s/it]  9%|▊         | 679/7890 [17:31<2:49:18,  1.41s/it]  9%|▊         | 680/7890 [17:32<2:50:14,  1.42s/it]  9%|▊         | 681/7890 [17:34<2:49:12,  1.41s/it]  9%|▊         | 682/7890 [17:36<3:05:23,  1.54s/it]  9%|▊         | 683/7890 [17:37<2:50:03,  1.42s/it]  9%|▊         | 684/7890 [17:38<2:38:04,  1.32s/it]  9%|▊         | 685/7890 [17:39<2:31:48,  1.26s/it]  9%|▊         | 686/7890 [17:40<2:31:24,  1.26s/it]  9%|▊         | 687/7890 [17:42<2:37:31,  1.31s/it]  9%|▊         | 688/7890 [17:43<2:40:32,  1.34s/it]  9%|▊         | 689/7890 [17:44<2:43:04,  1.36s/it]  9%|▊         | 690/7890 [17:46<2:46:44,  1.39s/it]  9%|▉         | 691/7890 [17:47<2:37:11,  1.31s/it]  9%|▉         | 692/7890 [17:48<2:29:58,  1.25s/it]  9%|▉         | 693/7890 [17:49<2:33:36,  1.28s/it]  9%|▉         | 694/7890 [17:51<2:36:26,  1.30s/it]  9%|▉         | 695/7890 [17:52<2:35:43,  1.30s/it]  9%|▉         | 696/7890 [17:53<2:40:19,  1.34s/it]  9%|▉         | 697/7890 [17:55<2:41:19,  1.35s/it]  9%|▉         | 698/7890 [17:56<2:38:10,  1.32s/it]  9%|▉         | 699/7890 [17:57<2:31:30,  1.26s/it]  9%|▉         | 700/7890 [17:58<2:27:17,  1.23s/it]                                                    {'loss': 0.1322, 'grad_norm': 1.2952720203217616, 'learning_rate': 1.9705215685415876e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 88448.0, 'epoch': 0.27}
  9%|▉         | 700/7890 [17:58<2:27:17,  1.23s/it]  9%|▉         | 701/7890 [18:00<2:40:17,  1.34s/it]  9%|▉         | 702/7890 [18:01<2:31:18,  1.26s/it]  9%|▉         | 703/7890 [18:02<2:28:36,  1.24s/it]  9%|▉         | 704/7890 [18:03<2:25:06,  1.21s/it]  9%|▉         | 705/7890 [18:05<2:46:27,  1.39s/it]  9%|▉         | 706/7890 [18:06<2:37:03,  1.31s/it]  9%|▉         | 707/7890 [18:08<2:45:29,  1.38s/it]  9%|▉         | 708/7890 [18:10<3:00:13,  1.51s/it]  9%|▉         | 709/7890 [18:11<2:56:27,  1.47s/it]  9%|▉         | 710/7890 [18:13<3:05:20,  1.55s/it]  9%|▉         | 711/7890 [18:14<2:49:01,  1.41s/it]  9%|▉         | 712/7890 [18:15<2:47:25,  1.40s/it]  9%|▉         | 713/7890 [18:17<2:49:24,  1.42s/it]  9%|▉         | 714/7890 [18:18<2:38:48,  1.33s/it]  9%|▉         | 715/7890 [18:19<2:30:59,  1.26s/it]  9%|▉         | 716/7890 [18:20<2:37:53,  1.32s/it]  9%|▉         | 717/7890 [18:22<2:37:36,  1.32s/it]  9%|▉         | 718/7890 [18:23<2:29:53,  1.25s/it]  9%|▉         | 719/7890 [18:24<2:24:55,  1.21s/it]  9%|▉         | 720/7890 [18:26<2:39:30,  1.33s/it]  9%|▉         | 721/7890 [18:27<2:44:46,  1.38s/it]  9%|▉         | 722/7890 [18:28<2:37:52,  1.32s/it]  9%|▉         | 723/7890 [18:29<2:34:20,  1.29s/it]  9%|▉         | 724/7890 [18:31<2:35:23,  1.30s/it]  9%|▉         | 725/7890 [18:32<2:39:39,  1.34s/it]                                                    {'loss': 0.1322, 'grad_norm': 1.4280532084310547, 'learning_rate': 1.9681139616198712e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 75456.0, 'epoch': 0.28}
  9%|▉         | 725/7890 [18:32<2:39:39,  1.34s/it]  9%|▉         | 726/7890 [18:33<2:31:23,  1.27s/it]  9%|▉         | 727/7890 [18:35<2:55:59,  1.47s/it]  9%|▉         | 728/7890 [18:37<2:53:31,  1.45s/it]  9%|▉         | 729/7890 [18:38<2:50:57,  1.43s/it]  9%|▉         | 730/7890 [18:39<2:50:22,  1.43s/it]  9%|▉         | 731/7890 [18:41<2:38:50,  1.33s/it]  9%|▉         | 732/7890 [18:42<2:33:21,  1.29s/it]  9%|▉         | 733/7890 [18:43<2:35:15,  1.30s/it]  9%|▉         | 734/7890 [18:45<2:40:40,  1.35s/it]  9%|▉         | 735/7890 [18:46<2:47:35,  1.41s/it]  9%|▉         | 736/7890 [18:47<2:35:52,  1.31s/it]  9%|▉         | 737/7890 [18:48<2:27:25,  1.24s/it]  9%|▉         | 738/7890 [18:50<2:36:02,  1.31s/it]  9%|▉         | 739/7890 [18:51<2:40:07,  1.34s/it]  9%|▉         | 740/7890 [18:53<2:44:16,  1.38s/it]  9%|▉         | 741/7890 [18:54<2:42:07,  1.36s/it]  9%|▉         | 742/7890 [18:55<2:44:51,  1.38s/it]  9%|▉         | 743/7890 [18:57<2:46:03,  1.39s/it]  9%|▉         | 744/7890 [18:58<2:45:10,  1.39s/it]  9%|▉         | 745/7890 [19:00<2:58:07,  1.50s/it]  9%|▉         | 746/7890 [19:01<2:54:16,  1.46s/it]  9%|▉         | 747/7890 [19:03<2:48:11,  1.41s/it]  9%|▉         | 748/7890 [19:04<2:48:42,  1.42s/it]  9%|▉         | 749/7890 [19:05<2:51:35,  1.44s/it] 10%|▉         | 750/7890 [19:07<2:50:07,  1.43s/it]                                                    {'loss': 0.1252, 'grad_norm': 1.2958690004663391, 'learning_rate': 1.9656135308818572e-05, 'skipped_iter': 0, 'num_zeros_in_grad': 78232.0, 'epoch': 0.29}
 10%|▉         | 750/7890 [19:07<2:50:07,  1.43s/it] 10%|▉         | 751/7890 [19:08<2:56:04,  1.48s/it] 10%|▉         | 752/7890 [19:10<2:54:11,  1.46s/it] 10%|▉         | 753/7890 [19:12<2:58:56,  1.50s/it] 10%|▉         | 754/7890 [19:13<2:50:20,  1.43s/it] 10%|▉         | 755/7890 [19:14<2:37:22,  1.32s/it] 10%|▉         | 756/7890 [19:15<2:37:24,  1.32s/it]